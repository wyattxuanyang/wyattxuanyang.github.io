<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="文献阅读 等变图神经网络 Frame AveragingICLR 2022 ORAL  目录：  概念说明和理论证明 实验细节设置、图的背景知识和代码分析  补充知识：  WL算法判断图同构 GIN网络 message-passing-GNN   参考资料： 【GNN】WL-test：GNN 的性能上界 (qq.com) KNN算法（k近邻算法）原理及总结-CSDN博客 搞懂DGCNN，这篇就够了">
<meta property="og:type" content="article">
<meta property="og:title" content="fa">
<meta property="og:url" content="http://example.com/2024/08/27/fa/index.html">
<meta property="og:site_name" content="轩阳的赛博居所">
<meta property="og:description" content="文献阅读 等变图神经网络 Frame AveragingICLR 2022 ORAL  目录：  概念说明和理论证明 实验细节设置、图的背景知识和代码分析  补充知识：  WL算法判断图同构 GIN网络 message-passing-GNN   参考资料： 【GNN】WL-test：GNN 的性能上界 (qq.com) KNN算法（k近邻算法）原理及总结-CSDN博客 搞懂DGCNN，这篇就够了">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-08-27T15:43:08.000Z">
<meta property="article:modified_time" content="2024-08-27T16:01:26.923Z">
<meta property="article:author" content="Wyatt">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2024/08/27/fa/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>fa | 轩阳的赛博居所</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">轩阳的赛博居所</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Sing once again with me!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/08/27/fa/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wyatt">
      <meta itemprop="description" content="相关功能正在完善中...关于以下问题，欢迎随时分享你的经验:  1. 我应该怎样在grouppicture的环境下使得图片标题正常显示?已经尝试过的方法包括但不限于使用html语言和markdown内置的方式直接包裹m, 更改grouppicture的css文件, etc.2. 有什么推荐的欧洲旅行住宿地址？">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="轩阳的赛博居所">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          fa
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-08-27 23:43:08" itemprop="dateCreated datePublished" datetime="2024-08-27T23:43:08+08:00">2024-08-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-08-28 00:01:26" itemprop="dateModified" datetime="2024-08-28T00:01:26+08:00">2024-08-28</time>
              </span>

          
            <span id="/2024/08/27/fa/" class="post-meta-item leancloud_visitors" data-flag-title="fa" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/08/27/fa/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/08/27/fa/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="文献阅读-等变图神经网络-Frame-Averaging"><a href="#文献阅读-等变图神经网络-Frame-Averaging" class="headerlink" title="文献阅读 等变图神经网络 Frame Averaging"></a>文献阅读 等变图神经网络 Frame Averaging</h1><p><strong>ICLR 2022 ORAL</strong></p>
<hr>
<p><strong>目录：</strong></p>
<ol>
<li><strong>概念说明和理论证明</strong></li>
<li><strong>实验细节设置、图的背景知识和代码分析</strong></li>
</ol>
<p>补充知识：</p>
<ol>
<li>WL算法判断图同构</li>
<li>GIN网络</li>
<li>message-passing-GNN</li>
</ol>
<hr>
<p>参考资料：</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/oKOUcvl-SIXMI8iBK76IHQ">【GNN】WL-test：GNN 的性能上界 (qq.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_74405427/article/details/133714384">KNN算法（k近邻算法）原理及总结-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/267895014">搞懂DGCNN，这篇就够了！论文及代码完全解析 - 知乎 (zhihu.com)</a></p>
<p>点云网络没有仔细看</p>
<hr>
<p>还可以读什么呢？</p>
<p>SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks</p>
<p>Vector Neurons: A General Framework for SO(3)-Equivariant Networks（矢量神经元？）</p>
<p>On the Universality of Rotation Equivariant Point Cloud Networks <strong>ICLR2020</strong></p>
<p>GemNet: Universal Directional Graph Neural Networks for Molecules <strong>NIPS2021</strong></p>
<p>Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs <strong>ICLR2022</strong></p>
<hr>
<ul>
<li>等变性：称函数$\phi$在群空间G上具有等变性，若<br>$$<br>\phi(\rho_X(g)x) &#x3D; \rho_Y(g)\phi(x),\forall g \in G<br>$$<br>其中，$\rho_X,\rho_Y$是输入和输出空间的群表示。</li>
</ul>
<p>群表示：把一个群通过群同态，映射到一个可逆线性变换（可逆矩阵）中。</p>
<p>赋范线性空间：定义了范数的线性空间。赋范线性空间的性质比距离空间强很多，因为它上面一定可以定义距离$d(x,y) &#x3D; ||x-y||$</p>
<p>映射$\phi:V\rightarrow \mathbb{R}, \Phi:V\rightarrow W$。考虑一个群$G&#x3D;{g}$，通过群表示$\rho_1；G\rightarrow GL(V)$  和$\rho_2；G\rightarrow GL(W)$作用于V,W上。</p>
<p>目标：让$\phi$成为“不变的”函数，即$\phi(\rho_1(g)X) &#x3D; \phi(X)$；让$\Phi$成为等变的函数，即$\Phi(\rho_1(g)X) &#x3D; \rho_2(g)\Phi(X)$。这里之所以一个是映射到R，一个是映射到W，是因为前者尝尝对应于分类问题，后者则经常对应于点云处理等，需要输出以（和输入）相同方式旋转的问题。</p>
<p>之前的研究说明，通过数据在整个G上进行变换，然后取</p>
<p>$$<br>\psi(X)&#x3D;\frac{1}{|G|} \sum_{g \in G} \phi\left(g^{-1} \cdot X\right) \quad \text { or } \quad \Psi(X)&#x3D;\frac{1}{|G|} \sum_{g \in G} g \cdot \Phi\left(g^{-1} \cdot X\right)<br>$$</p>
<p>就可以得到不变或者等变的网络。然而，FA只用了G的一个子集。</p>
<ul>
<li><strong>Def1. 一个frame被定义为一个取值为集合的函数，将输入空间V映射到G的子集的集合，也就是每个输入空间的元素x映到一个G的子集。其具有两个性质（注意，下面的F(X)是一个集合，里面的每个元素都是G中的元素）</strong><ul>
<li>G-equivariant    $\mathcal{F}(\rho_1(g)X) &#x3D; g\mathcal{F}(X), \forall X \in V, g\in G$。其中gF(X)就是g对F(x)中每个元素进行左乘后得到的元素集合，上式等号指代集合意义的相等。</li>
<li>有界性   通过X定义的frame，通过frame里的元素g定义的ρ(g)，ρ(g)是有界的</li>
</ul>
</li>
</ul>
<p>因此可以用这个G的子集，来定义我们的平均化操作$&lt;\phi&gt;_F$</p>
<p>$$<br>\begin{aligned}<br>\langle\phi\rangle_{\mathcal{F}}(X) &amp; &#x3D;\frac{1}{|\mathcal{F}(X)|} \sum_{g \in \mathcal{F}(X)} \phi\left(\rho_{1}(g)^{-1} X\right) \<br>\langle\Phi\rangle_{\mathcal{F}}(X) &amp; &#x3D;\frac{1}{|\mathcal{F}(X)|} \sum_{g \in \mathcal{F}(X)} \rho_{2}(g) \Phi\left(\rho_{1}(g)^{-1} X\right)<br>\end{aligned}<br>$$</p>
<ul>
<li><strong>Theo1.这样得到的frame是具有G-不变&#x2F;等变的。也就是只用这个F(x)进行的映射构造，和利用整个G进行构造，效果相同。</strong></li>
</ul>
<p>不变性的证明：<br>$$<br>\text{prove of: }&lt;\phi&gt;<em>F(\rho_1(g’)X) &#x3D; &lt;\phi&gt;<em>F(X)\<br>\text{ 即证明利用帧平均得到的映射，对于G中每一个元素都是不变的。}<br>\forall g’\in G, \<br>\begin{aligned}&lt;\phi&gt;<em>F(\rho_1(g’)X)&#x3D;&amp;\frac{1}{F(X)}\sum</em>{g\in g’F(X)}\phi(\rho_1(g)^{-1}\rho_1(g’)X) \quad &amp;\text{（定义）}\<br>&#x3D;&amp;\frac{1}{F(X)}\sum</em>{g\in F(X)}\phi(\rho_1(g’g)^{-1}\rho_1(g’)X) \quad &amp;\<br>&#x3D;&amp;\frac{1}{F(X)}\sum</em>{g\in F(X)}\phi(\rho_1(g)^{-1}X)\quad &amp;\text{（群同态保持乘法）}\<br>&#x3D;&amp;&lt;\phi&gt;_F(X)&amp;<br>\end{aligned}<br>$$<br>第二个等号是因为，F(X)中每个元素是g，让其中的每个元素g左乘g’，就相当于一个g’F(X)的元素。第三个等号是因为群同态保持乘法，而g’g的逆是g逆g’逆，因为逆元唯一。</p>
<p>等变性的证明是类似的：</p>
<p>$$<br>\begin{aligned}<br>\langle\Phi\rangle_{\mathcal{F}}(\rho_{1}(g^{\prime})X)&amp; &#x3D;\frac{1}{|\mathcal{F}(X)|}\sum_{g\in g^{\prime}\mathcal{F}(X)}\rho_{2}(g)\Phi(\rho_{1}(g)^{-1}\rho_{1}(g^{\prime})X) \<br>&amp;&#x3D;\frac{1}{|\mathcal{F}(X)|}\sum_{g\in\mathcal{F}(X)}\rho_{2}(g’g)\Phi(\rho_{1}(g’g)^{-1}\rho_{1}(g’)X) \<br>&amp;&#x3D;\rho_{2}(g^{\prime})\frac{1}{|\mathcal{F}(X)|}\sum_{g\in\mathcal{F}(X)}\rho_{2}(g)\Phi(\rho_{1}(g)^{-1}X) \<br>&amp;&#x3D;\rho_{2}(g^{\prime})\langle\Phi\rangle_{\mathcal{F}}(X)<br>\end{aligned}<br>$$</p>
<p>不变映射，是等变映射在选择 W &#x3D; R 和平凡表示 ρ2(g) ≡ 1 时的特例。</p>
<p>进一步地，如果某个网络架构已经对于某个对称群H具有了不变性或等变性，则可以将这种不变性或等变性扩展到更大的对称群H×G。</p>
<p>预定义：设$\rho_1,\tau_1$是G和H在V上的表示，$\rho_2,\tau_2$同理是GH在W上的表示。</p>
<ol>
<li>称表示$\rho_1,\tau_1$是可交换的，如果对于所有的g,h,X（属于V）有$\rho_1(g)\tau_1(h)X&#x3D;\tau_1(h)\rho_1(g)X$。如果他们可交换，则可以定义GH上的一个表示$\gamma_1(h,g)&#x3D;\rho_1(g)\tau_1(h)$</li>
<li>F(X)对于H来说是不变的，即$\mathcal{F}(\tau_1(h)X) &#x3D; \mathcal{F}(X)$</li>
</ol>
<ul>
<li><strong>Theo2. 设F是H-不变，G-等变的：</strong><ul>
<li><strong>如果$\phi$是H-不变的，表示$\rho_1,\tau_1$是可交换的，那么$&lt;\phi&gt;_F$是G×H不变的</strong></li>
<li><strong>如果$\Phi$是H-等变的，表示$\rho_i,\tau_i,i&#x3D;1,2$是可交换的，那么$&lt;\phi&gt;_F$是G×H等变的</strong></li>
</ul>
</li>
</ul>
<p>也可以通过右乘来定义等变性&#x2F;不变性。</p>
<p>更加高效地计算不变性frame：</p>
<p>稳定子群：作用于X上而保持X不变的元素。$G_X &#x3D; {g\in G|\rho_1(g)X&#x3D;X}$</p>
<p>等价关系：如果$F(X)$中两个元素可以通过$G_X$中的元素相互转换，$g^{-1}h\in G_X$，则它们是等价的。所有和g等价的F(X)中元素记为等价类，也就是一个轨道[g]。</p>
<ul>
<li><strong>Theo3. $\mathcal{F}(X)$是所有等大小轨道[g]的无交并，$[g]\in \mathcal{F}(X)&#x2F;G_X$</strong></li>
</ul>
<p>说明这个F(X)大小至少是X处稳定子群的大小。另外由于[g]轨道上每个元素对于X作用后，通过$\phi$会产生一样的效果：</p>
<p>$$<br>h&#x3D;rg, r\in G_{X}, \text{and}\ \ \phi(\rho_{1}(h)^{-1}X)&#x3D;\phi(\rho_{1}(g)^{-1}\rho_{1}(r)^{-1}X)&#x3D;\phi(\rho_{1}(g)^{-1}X)<br>$$</p>
<p>因此只需要每个轨道挑一个出来计算&#x2F;构造不变映射即可。（如何挑出不同的轨道？这个过程是否仍然需要计算？还是说依赖于具体的群的类型）</p>
<ul>
<li><strong>推论：$\mathcal{F}(X)$是个等变的，$g\in \mathcal{F}(X)$是均匀的，那么轨道$[g]\in \mathcal{F}(X)&#x2F;G_X$也是均匀的</strong></li>
</ul>
<p>好像下面这个推论是对上面的问题的回答。既然每条轨道大小相等，那么我在所有轨道组成的并集也就是整个F(X)上随机取元素，取出的元素在每个轨道上的概率相等。</p>
<p>这个过程中唯一的问题就是需要计算出轨道的数量，我推测这里只要能算出frame的大小|F(X)|和稳定子群$G_X$的大小就行了。可以用抽样方法估计稳定子群的大小，即随机找一些g看他们作用于X之后X是否保持不变。</p>
<p>运用了FA的映射，表达能力得到了保证。</p>
<ul>
<li><p><strong>Theo4. 一个运用了FA方法（假设其在K上是有界的，即K中的每个X，F(X)有界）的网络&#x2F;映射（记为映射1），其与任意一个G-等变网络&#x2F;映射（记为映射2）的结果的差距，可以被两个映射在$K_F$上的上界一致控制。</strong>（映射1运用了FA之后也变成G-等变了）</p>
<p>这个证明很有意思</p>
<p>A.5 PROOF OF THEOREM 4</p>
</li>
</ul>
<p>$Proof.$ Let $\Psi\in\mathcal{C}(V,W)$ be an arbitrary $G$ equivariant function, $\mathcal{F}$ a bounded $G$ equivariant frame</p>
<p>over a frame-finite domain $K.$ Let $c&gt; \dot{0}$be the constant from Definition 1. For arbitrary $X\in K$,<br>$$<br>\begin{aligned}\left|\Psi(X)-\langle\Phi\rangle_{\mathcal{F}}(X)\right|<em>{W}&amp;&#x3D;\left|\langle\Psi\rangle</em>{\mathcal{F}}(X)-\langle\Phi\rangle_{\mathcal{F}}(X)\right|<em>{W}\&amp;\leq\frac{1}{|\mathcal{F}(X)|}\sum</em>{g\in\mathcal{F}(X)}\left|\rho_{2}(g)\Psi(\rho_{1}(g)^{-1}X)-\rho_{2}(g)\Phi(\rho_{1}(g)^{-1}X)\right|<em>{W}\&amp;\leq\max</em>{g\in\mathcal{F}(X)}\left|\rho_{2}(g)\right|<em>{\mathrm{op}}\left|\Psi-\Phi\right|</em>{K_{F},W}\&amp;\leq c\left|\Psi-\Phi\right|<em>{K</em>{\mathcal{F},W}}\end{aligned}<br>$$</p>
<p>where in the first equality we used the fact that $\langle\Psi\rangle_{\mathcal{F}}&#x3D;\Psi$ since $\Psi$ is already G equivariant.</p>
<h2 id="实验部分："><a href="#实验部分：" class="headerlink" title="实验部分："></a>实验部分：</h2><p>重点看了后两个实验的设置和代码</p>
<ul>
<li>3D点云-欧几里得变换群</li>
</ul>
<p>群：rotation &amp; reflection 群O(d) x translation 群T(d)，或者SE(d)&#x3D;SO(d)xT(d)，其中SO(d)只包含旋转</p>
<p>Frame：通过PCA定义。<br>$$<br>t&#x3D;\frac{1}{n}X^T 1 \in R^{d},\text{X的行（第一维）是数据量n，第二维是特征维度}<br>\ \text{t是一个列向量，每个维度记录X中所有数据在这个维度的均值}\<br>C&#x3D;(X-1t^T)^T(X-1t^T),\text{协方差矩阵}\<br>\text{有特征值}\lambda_1-\lambda_n，对应特征向量v_1-v_n\<br>F(X)&#x3D;{([\alpha_1v_1,\dots\alpha_nv_n<br>],t)|\alpha_i&#x3D;1或-1}\subset E(d)\<br>$$<br>这里假设协方差矩阵具有simple spectrum，也就是非重复特征值，“几乎处处”可以定义F(X)</p>
<p>F也是Sn不变的。</p>
<ul>
<li>图(graph)-置换群Sn</li>
</ul>
<p>这一块是最难看懂的，涉及对图的谱表示，WL可区分性及GIN对WL的性能逼近等</p>
<p><strong>背景知识补充1：GIN网络和图同构问题的算法性能上界</strong></p>
<p>对图同构问题（NP问题）目前最有效的算法是Weisfeiler-Lehman 算法，可以在准多项式时间求解。WL算法解决的问题是：比较两个图是否同构。概括来说，每次迭代中，将一个点表示成**{这个点的编号：周围的点的编号}**的形式（这里的“编号”相当于点的种类），然后为每一种表示构建唯一的hash编码。迭代多轮或者图收敛后，停止迭代。</p>
<p>WL算法和图网络类似：每轮都是聚合+更新。</p>
<p>WL-test算法是GNN的性能上界。因为可以证明：如果能用GNN区分两个图是否同构，则WL-test一定可以。证明要点在于WL-test算法的<strong>单射性质</strong>。</p>
<p>构造出一个图网络，逼近这个上界。构造出的网络就是GIN。论文中定理指出了，图神经网络在满足这些条件时，能够将WL测试认为是非同构图的两个图和映射到不同的embedding</p>
<p>①按照$h_v^k&#x3D;\phi(h_v^{k-1},f({h_u^{k-1}:u\in N(v)})$进行每轮的aggregate；</p>
<p>②作用在图级的READOUT函数也是单射的 </p>
<p>后面涉及了一些关于可数集函数映射和利用MLP逼近表示的定理，没有细看。总之最后得到的每层<strong>AGGREGATE</strong>更新方式是：<br>$$<br>h_v^k &#x3D; \text{MLP}^k((1+\epsilon^k )h_v^{k-1}+\sum_{u\in N(v)}h_u^{k-1})<br>$$<br><strong>READOUT</strong>为对每次迭代得到的所有节点的特征求和得到该轮迭代的图特征，然后再拼接起每一轮迭代的图特征来得到最终的图特征<br>$$<br>h_G&#x3D;\text{CONCACT}(\text{SUM}(h_v^k|v\in G)|k&#x3D;0\cdots K)<br>$$<br>相比其他图网络的单层传播，多层感知机拟合能力强；相比mean和max，sum不会混淆一些结构。</p>
<p>代码说明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GinNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GinNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        neuron=<span class="number">64</span></span><br><span class="line">        r1=np.random.uniform()</span><br><span class="line">        r2=np.random.uniform()</span><br><span class="line">        r3=np.random.uniform()</span><br><span class="line"></span><br><span class="line">        nn1 = Sequential(Linear(dataset.num_features, neuron))</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GINConv(nn1,eps=r1,train_eps=<span class="literal">True</span>)        </span><br><span class="line"></span><br><span class="line">        nn2 = Sequential(Linear(neuron, neuron))</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = GINConv(nn2,eps=r2,train_eps=<span class="literal">True</span>)        </span><br><span class="line"></span><br><span class="line">        nn3 = Sequential(Linear(neuron, neuron))</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = GINConv(nn3,eps=r3,train_eps=<span class="literal">True</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.fc1 = torch.nn.Linear(neuron, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, data</span>):</span><br><span class="line"></span><br><span class="line">        x=data.x </span><br><span class="line">        edge_index=data.edge_index</span><br><span class="line">        </span><br><span class="line">        x = torch.tanh(<span class="variable language_">self</span>.conv1(x, edge_index))</span><br><span class="line">        x = torch.tanh(<span class="variable language_">self</span>.conv2(x, edge_index))        </span><br><span class="line">        x = torch.tanh(<span class="variable language_">self</span>.conv3(x, edge_index))             </span><br><span class="line"></span><br><span class="line">        x = global_add_pool(x, data.batch)</span><br><span class="line">        x = torch.tanh(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引数组的随机置换，相当于Sn里面元素的作用</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">genrate_perm</span>(<span class="params">perm_idx</span>):</span><br><span class="line">    perm = [np.random.permutation(perm) <span class="keyword">for</span> perm <span class="keyword">in</span> perm_idx]</span><br><span class="line">    <span class="keyword">return</span> perm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据边索引构造一个n x n的邻接矩阵，然后将该矩阵展平输出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_A</span>(<span class="params">edge_index, n</span>):</span><br><span class="line">    A=np.zeros((n,n),dtype=np.float32)</span><br><span class="line">    A[edge_index[<span class="number">0</span>],edge_index[<span class="number">1</span>]]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(A.flatten())</span><br></pre></td></tr></table></figure>

<p><code>sort_fn_laplacian</code>的作用是根据拉普拉斯矩阵的特征值，返回节点排序索引和对应的特征向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sort_fn_laplacian</span>(<span class="params">x,edge_index</span>):</span><br><span class="line">    <span class="comment"># 构建拉普拉斯矩阵 </span></span><br><span class="line">    L_e, L_w = torch_geometric.utils.get_laplacian(edge_index)</span><br><span class="line">    L = np.zeros((x.shape[<span class="number">0</span>],x.shape[<span class="number">0</span>]),dtype=np.float32)</span><br><span class="line">    L[L_e[<span class="number">0</span>],L_e[<span class="number">1</span>]]=L_w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算特征值和特征向量，按照特征值从大到小返回</span></span><br><span class="line">    evals, evecs = np.linalg.eigh(L)</span><br><span class="line">    <span class="comment"># ----- create sorting criterion -----  </span></span><br><span class="line">    <span class="comment"># 使用unique函数找出所有唯一的特征值，以及每个特征值的起始索引和重数</span></span><br><span class="line">    unique_vals, evals_idx, evals_mult = np.unique(evals, return_counts=<span class="literal">True</span>, return_index=<span class="literal">True</span>) <span class="comment"># get eigenvals multiplicity</span></span><br><span class="line">        </span><br><span class="line">    chosen_evecs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果特征值不重复，就取出特征向量的绝对值，加入chosen_evecs</span></span><br><span class="line">    <span class="comment"># 如果特征值重复，计算子空间基向量的平方和的平方根，加入chosen_evecs</span></span><br><span class="line">    <span class="comment"># 注意到，即使是重复特征值（重复了k次比如说），也会计算出k个向量加入chosen_evecs</span></span><br><span class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(evals_idx)):</span><br><span class="line">        <span class="keyword">if</span> evals_mult[ii] == <span class="number">1</span>:</span><br><span class="line">            chosen_evecs.append(np.<span class="built_in">abs</span>(evecs[:,evals_idx[ii]]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            eigen_space_start_idx = evals_idx[ii]</span><br><span class="line">            eigen_space_size = evals_mult[ii]</span><br><span class="line">            eig_space_basis = evecs[:, eigen_space_start_idx:(eigen_space_start_idx+eigen_space_size)]</span><br><span class="line">            chosen_evecs.append(np.sqrt((eig_space_basis ** <span class="number">2</span>).<span class="built_in">sum</span>(<span class="number">1</span>)))</span><br><span class="line">		</span><br><span class="line">		<span class="comment"># 将选择的特征向量堆叠成一个矩阵，保留两位小数</span></span><br><span class="line">    chosen_evecs = np.stack(chosen_evecs, axis=<span class="number">1</span>).<span class="built_in">round</span>(decimals=<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># lexsort根据键排序，返回排序索引sort_idx和chosen_evecs。[::-1]是倒序取所有的索引</span></span><br><span class="line">    sort_idx = np.lexsort([col <span class="keyword">for</span> col <span class="keyword">in</span> chosen_evecs.transpose()[::-<span class="number">1</span>]]) <span class="comment"># consider regular sort</span></span><br><span class="line">    <span class="keyword">return</span> sort_idx, chosen_evecs</span><br></pre></td></tr></table></figure>

<p><code>SortFrame</code> 类是用于处理和转换图数据，按照基于拉普拉斯矩阵的特征向量对图中的节点进行排序。传入的data首先需要经过pre_transform，然后进行结点和边的重排序（使用sort_fn_laplacian），返回排序后的data。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SortFrame</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,device, pre_transform,sort_fn=sort_fn_laplacian</span>):</span><br><span class="line">        <span class="variable language_">self</span>.pre_transform = pre_transform</span><br><span class="line">        <span class="variable language_">self</span>.sort_fn = sort_fn</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, data</span>):</span><br><span class="line">        data = <span class="variable language_">self</span>.pre_transform(data)</span><br><span class="line">        <span class="comment"># data = Data(**data.__dict__)</span></span><br><span class="line">        sort_idx, to_sort = <span class="variable language_">self</span>.sort_fn(data.x, data.edge_index)</span><br><span class="line">        sorted_x = to_sort[sort_idx,:]</span><br><span class="line">        unique_rows, dup_rows_idx, dup_rows_mult = np.unique(sorted_x, axis=<span class="number">0</span>, return_index=<span class="literal">True</span>, return_counts=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        perm_start_idx = dup_rows_idx[dup_rows_mult!=<span class="number">1</span>]</span><br><span class="line">        perm_size = dup_rows_mult[dup_rows_mult!=<span class="number">1</span>]</span><br><span class="line">        perm_idx = []</span><br><span class="line">        <span class="keyword">for</span> ii <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(perm_size)):</span><br><span class="line">            perm_idx.append(np.arange(perm_start_idx[ii], perm_start_idx[ii]+perm_size[ii]))</span><br><span class="line">        data.perm_idx = perm_idx</span><br><span class="line">        data.sort_idx = sort_idx</span><br><span class="line">        data.size = data.x.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<p><code>SampleFrame8C</code>类：生成一组新的图数据样本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SampleFrame8C</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,size=<span class="number">64</span>,sample_size=<span class="number">10</span>, GA=<span class="literal">False</span>,MLP=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.sample_size = sample_size</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.counter=<span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.GA=GA</span><br><span class="line">        <span class="variable language_">self</span>.<span class="built_in">id</span> = <span class="keyword">not</span> MLP</span><br><span class="line">        <span class="variable language_">self</span>.MLP = MLP</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply_permutation</span>(<span class="params">self, perm_idx, edge_index, x</span>):</span><br><span class="line">        inv_perm_idx = np.zeros_like(perm_idx)</span><br><span class="line">        inv_perm_idx[perm_idx] = np.arange(perm_idx.shape[<span class="number">0</span>]) <span class="comment">#根据perm_idx.shape[0]返回等差索引</span></span><br><span class="line">        sorted_edge_index = torch.tensor(inv_perm_idx[edge_index])</span><br><span class="line">        sorted_x = x[perm_idx,:]</span><br><span class="line">        <span class="keyword">return</span> sorted_edge_index, sorted_x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">permute_with_perm</span>(<span class="params">self,perm_idx, perm, sort_idx, edge_index, x</span>):</span><br><span class="line">        inv_sort_idx = np.zeros_like(sort_idx)</span><br><span class="line">        inv_sort_idx[sort_idx] = np.arange(sort_idx.shape[<span class="number">0</span>])</span><br><span class="line">        inv_sort_idx[sort_idx[<span class="built_in">list</span>(itertools.chain(*perm_idx))]] = <span class="built_in">list</span>(itertools.chain(*perm))</span><br><span class="line">        sorted_edge_index = inv_sort_idx[edge_index]</span><br><span class="line">        cur_sort_idx = np.zeros_like(sort_idx)</span><br><span class="line">        cur_sort_idx[inv_sort_idx] = np.arange(sort_idx.shape[<span class="number">0</span>])</span><br><span class="line">     </span><br><span class="line">        sorted_x_feat = x[cur_sort_idx,:]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sorted_edge_index, sorted_x_feat</span><br><span class="line">     </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self,data</span>):</span><br><span class="line">        n = data.x.shape[<span class="number">0</span>]</span><br><span class="line">        m = <span class="variable language_">self</span>.size</span><br><span class="line">        x = data.x</span><br><span class="line">        d = x.shape[<span class="number">1</span>]</span><br><span class="line">        edge_index = data.edge_index</span><br><span class="line">        sort_idx = data.sort_idx</span><br><span class="line">        perm_idx = data.perm_idx</span><br><span class="line">        x_arr = []</span><br><span class="line">        e_arr = []</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> perm_idx:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.GA:</span><br><span class="line">                sorted_edge_index, sorted_x = <span class="variable language_">self</span>.apply_permutation(np.random.permutation(n), edge_index.clone(), x.clone())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sorted_edge_index, sorted_x = <span class="variable language_">self</span>.apply_permutation(sort_idx, edge_index.clone(), x.clone())</span><br><span class="line">                data.edge_index = sorted_edge_index.detach()</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.MLP:</span><br><span class="line">                new_x = generate_A(sorted_edge_index,m).unsqueeze(<span class="number">0</span>)</span><br><span class="line">                data = new_x,data.y</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.<span class="built_in">id</span>:</span><br><span class="line">                    data.x = torch.cat([sorted_x.detach(),torch.eye(n, dtype=x.dtype),torch.zeros((n,m-n), dtype=x.dtype)],<span class="number">1</span>).clone()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    data.x = sorted_x.detach()</span><br><span class="line">                data.edge_index = sorted_edge_index</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.sample_size):</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.GA:</span><br><span class="line">                    sorted_edge_index, sorted_x = <span class="variable language_">self</span>.apply_permutation(np.random.permutation(n), edge_index.clone(), x.clone())</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    perm = genrate_perm(perm_idx)</span><br><span class="line">                    sorted_edge_index, sorted_x = <span class="variable language_">self</span>.permute_with_perm(perm_idx, perm, sort_idx, edge_index.clone(), x.clone())</span><br><span class="line">                    sorted_edge_index = torch.from_numpy(sorted_edge_index)</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.MLP:</span><br><span class="line">                    x_arr.append(generate_A(sorted_edge_index,m))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.<span class="built_in">id</span>:</span><br><span class="line">                        x_arr.append(torch.cat([sorted_x.detach(),torch.eye(n, dtype=x.dtype),torch.zeros((n,m-n), dtype=x.dtype)],<span class="number">1</span>).clone())</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        x_arr.append(sorted_x.detach())</span><br><span class="line">                    e_arr.append(torch.tensor(sorted_edge_index.clone()) + (i*n))</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.MLP:</span><br><span class="line">                data = torch.stack(x_arr,dim=<span class="number">0</span>), data.y</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                data.x = torch.cat(x_arr,<span class="number">0</span>).detach()</span><br><span class="line">                data.edge_index = torch.cat(e_arr,<span class="number">1</span>).detach()</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<ul>
<li>nbody</li>
</ul>
<p>群：与3D点云的一致（利用结点特征计算协方差矩阵，得到仿射变换）；同样可以证明这个群里的元素和置换群中的元素可交换，因此如果在一个Sn-invariant的backbone（映射）上运用F(X)，得到的映射仍然是Sn-invariant的。</p>
<p>backbone：massage-passing-GNN，是Sn-invariant的，$\phi_{d,d’}:R^{n \times d}\times R^{n\times n} \rightarrow R^{n \times d’}\times R^{n\times n}$，构造backbone为$&lt;\phi_{3d’,3}^{(l)}&gt;_F \circ \cdots\circ&lt;\phi_{3d’,3d’}^{(i)}&gt;_F\circ\cdots&lt;\phi_{3,3d’}^{(1)}&gt;_F(X)$。</p>
<p>输入：3+3维，表示初始位置和初始速度。从附录来看更新结点的时候还要考虑二者相互排斥还是相互吸引。φe和φh分别是两个MLP。</p>
<p>代码解释：</p>
<p><code>FA_GNN</code>网络架构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FA_GNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_nf, device=<span class="string">&#x27;cpu&#x27;</span>, act_fn=nn.SiLU(<span class="params"></span>), n_layers=<span class="number">4</span>, attention=<span class="number">0</span>, recurrent=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FA_GNN, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_nf = hidden_nf</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.n_layers = n_layers</span><br><span class="line">        <span class="variable language_">self</span>.dimension_reduce = nn.ModuleList()</span><br><span class="line">        <span class="comment"># GCL就是MLP：线性层+激活层。但是这个edge_in_nf是什么呢？</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n_layers):</span><br><span class="line">            <span class="variable language_">self</span>.add_module(<span class="string">&quot;gcl_%d&quot;</span> % i, GCL(<span class="variable language_">self</span>.hidden_nf, <span class="variable language_">self</span>.hidden_nf, <span class="variable language_">self</span>.hidden_nf, edges_in_nf=<span class="number">2</span>, act_fn=act_fn, attention=attention, recurrent=recurrent))</span><br><span class="line">        <span class="variable language_">self</span>.decoder = nn.Sequential(nn.Linear(hidden_nf, hidden_nf),</span><br><span class="line">                              act_fn,</span><br><span class="line">                              nn.Linear(hidden_nf, <span class="number">3</span>))</span><br><span class="line">                            </span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Sequential(nn.Linear(input_dim, hidden_nf))</span><br><span class="line">        <span class="variable language_">self</span>.to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, h, edges, edge_attr=<span class="literal">None</span></span>):</span><br><span class="line">        n_frame = <span class="number">8</span></span><br><span class="line">        n_nodes = <span class="number">5</span></span><br><span class="line">        batch_size = <span class="built_in">int</span>(h.shape[<span class="number">0</span>]/n_nodes)</span><br><span class="line">        n_h = h.shape[<span class="number">0</span>]</span><br><span class="line">        edges = expand_edge(edges, n_h, n_frame)</span><br><span class="line">        edge_attr = edge_attr.repeat(n_frame,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 原始的数据施加群作用，得到h，h再送入embedding得到嵌入</span></span><br><span class="line">        h, F_ops, center = create_frame(h, n_nodes)</span><br><span class="line">        h = <span class="variable language_">self</span>.embedding(h)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 看起来目的是让每一层都等变/不变。每一层都先过models.i层，得到h，然后计算这一层施加群作用的h</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_layers):</span><br><span class="line">            h, _ = <span class="variable language_">self</span>._modules[<span class="string">&quot;gcl_%d&quot;</span> % i](h, edges, edge_attr=edge_attr)</span><br><span class="line">            <span class="keyword">if</span> i &lt; (<span class="variable language_">self</span>.n_layers - <span class="number">1</span>):</span><br><span class="line">                <span class="comment"># transform equiv features extraction</span></span><br><span class="line">                h = invert_latent_frame(h, F_ops, batch_size, n_nodes, <span class="literal">None</span>)</span><br><span class="line">                <span class="comment"># compute new frame</span></span><br><span class="line">                h, F_ops, _ = create_latent_frame(h, n_nodes)</span><br><span class="line">               </span><br><span class="line"></span><br><span class="line">        h = <span class="variable language_">self</span>.decoder(h)</span><br><span class="line">        h = invert_frame(h, F_ops, n_nodes, center)</span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>

<p><code>create_frame</code>函数：输入是点的特征和点的数量，输出是施加了群作用的点特征（群作用指的是 协方差矩阵的特征值对应的八个旋转矩阵）</p>
<p><code>create_frame</code>和<code>create_latent_frame</code>的主要区别是，create_frame是输入数据进入网络之前进行的处理，latent则是在网络过程中进行的处理，因此输入数据的维度存在一些区别。（b<em>n</em>6和b<em>n</em>3）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_frame</span>(<span class="params">nodes, n_nodes</span>):</span><br><span class="line">    pnts = nodes[:,:<span class="number">3</span>]</span><br><span class="line">    v = nodes[:,<span class="number">3</span>:]</span><br><span class="line">    pnts = pnts.view(-<span class="number">1</span>, n_nodes, <span class="number">3</span>).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    v = v.view(-<span class="number">1</span>, n_nodes, <span class="number">3</span>).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    center = pnts.mean(<span class="number">2</span>,<span class="literal">True</span>)</span><br><span class="line">    pnts_centered = pnts - center</span><br><span class="line">    <span class="comment"># add noise</span></span><br><span class="line">    R = torch.bmm(pnts_centered,pnts_centered.transpose(<span class="number">1</span>,<span class="number">2</span>)) <span class="comment"># 协方差矩阵</span></span><br><span class="line">    lambdas,V_ = torch.symeig(R.detach().cpu(),<span class="literal">True</span>)</span><br><span class="line">    F =  V_.to(R) <span class="comment"># 这行在干嘛？to可以设备转换，也可以类型转换，结合上面detach来看应该是device</span></span><br><span class="line">    ops = torch.tensor([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                        [<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>],</span><br><span class="line">                        [<span class="number">1</span>,-<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                        [<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>],</span><br><span class="line">                        [-<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                        [-<span class="number">1</span>,<span class="number">1</span>,-<span class="number">1</span>],</span><br><span class="line">                        [-<span class="number">1</span>,-<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">                        [-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>]]).unsqueeze(<span class="number">1</span>).to(F)</span><br><span class="line">    F_ops = ops.unsqueeze(<span class="number">0</span>) * F.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 爱因斯坦求和：b*8*3*3.(交换最后两维) @ b*n_nodes*3.(交换最后两维)= b*8*n_nodes*3</span></span><br><span class="line">    <span class="comment"># b*8个  3*3（转置后）中，每个3*3都以行向量为特征向量；</span></span><br><span class="line">    <span class="comment"># b  个  3*n_nodes中，每列都是一个点的中心化坐标/速度</span></span><br><span class="line">    framed_input = torch.einsum(<span class="string">&#x27;boij,bpj-&gt;bopi&#x27;</span>,F_ops.transpose(<span class="number">2</span>,<span class="number">3</span>),(pnts - center).transpose(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    framed_v = torch.einsum(<span class="string">&#x27;boij,bpj-&gt;bopi&#x27;</span>,F_ops.transpose(<span class="number">2</span>,<span class="number">3</span>),(v).transpose(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    framed_input = framed_input.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    framed_input = torch.reshape(framed_input,(-<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    framed_v = framed_v.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    framed_v = torch.reshape(framed_v,(-<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    out = torch.cat([framed_input,framed_v],dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> out, F_ops.detach(), center.detach()</span><br></pre></td></tr></table></figure>

<p><code>invert_frame</code>函数：矩阵（群作用）作用于每一层的输出上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">invert_frame</span>(<span class="params">pnts, F_ops, n_nodes, center</span>):</span><br><span class="line">    pnts = pnts.view(<span class="number">8</span>, -<span class="number">1</span>, n_nodes,<span class="number">3</span>)</span><br><span class="line">    pnts = pnts.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    framed_input = torch.einsum(<span class="string">&#x27;boij,bopj-&gt;bopi&#x27;</span>,F_ops, pnts) </span><br><span class="line">    framed_input = framed_input.mean(<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">if</span> center <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        framed_input = framed_input + center.transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    framed_input = torch.reshape(framed_input,(-<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    <span class="keyword">return</span> framed_input</span><br></pre></td></tr></table></figure>

<p><code>invert_latent_frame</code>函数：这个是在decoder之前用的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">invert_latent_frame</span>(<span class="params">pnts, F_ops, batch_size, n_nodes, center</span>):</span><br><span class="line">    pnts = pnts.view(<span class="number">8</span>, batch_size, n_nodes, -<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    pnts = pnts.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    framed_input = torch.einsum(<span class="string">&#x27;boij,bopfj-&gt;bopfi&#x27;</span>,F_ops, pnts) </span><br><span class="line">    framed_input = framed_input.mean(<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">if</span> center <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        framed_input = framed_input + center.transpose(<span class="number">1</span>,<span class="number">2</span>).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">    framed_input = framed_input.contiguous()</span><br><span class="line">    framed_input = framed_input.view(batch_size,-<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> framed_input</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag"># 论文阅读</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/24/ml-pr/" rel="prev" title="机器学习&模式识别相关知识整理">
      <i class="fa fa-chevron-left"></i> 机器学习&模式识别相关知识整理
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%E7%AD%89%E5%8F%98%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Frame-Averaging"><span class="nav-number">1.</span> <span class="nav-text">文献阅读 等变图神经网络 Frame Averaging</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E9%83%A8%E5%88%86%EF%BC%9A"><span class="nav-number">1.1.</span> <span class="nav-text">实验部分：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wyatt</p>
  <div class="site-description" itemprop="description">相关功能正在完善中...关于以下问题，欢迎随时分享你的经验:  1. 我应该怎样在grouppicture的环境下使得图片标题正常显示?已经尝试过的方法包括但不限于使用html语言和markdown内置的方式直接包裹m, 更改grouppicture的css文件, etc.2. 有什么推荐的欧洲旅行住宿地址？</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wyatt</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '6pYHdmDKubDeNnQPIPU131iB-gzGzoHsz',
      appKey     : 'JDEEZHjqhKYfkORr8qmH7GqA',
      placeholder: "欢迎匿名的友善交流，不过我也希望大家给自己取个昵称，这样评论区看得出来谁是谁~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : 'https://6pyhdmdk.lc-cn-n1-shared.com'
    });
  }, window.Valine);
});
</script>

</body>
</html>
